{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (F-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "class SharedTaskConstants:\n",
    "    \"\"\"\n",
    "    Use these constants to interface with the data, not with the id2label used\n",
    "    inside the Huggingface models!!\n",
    "    \"\"\"\n",
    "    targets = ['validity', 'novelty']\n",
    "    validity_label_mapping = {\n",
    "        -1: \"not-valid\",\n",
    "        0: \"not-valid\",  # can be excluded since test set does not contain these\n",
    "        1: \"valid\",\n",
    "    }\n",
    "\n",
    "    novelty_label_mapping = {\n",
    "        -1: \"not-novel\",\n",
    "        0: \"not-novel\",  # can be excluded since test set does not contain these\n",
    "        1: \"novel\",\n",
    "    }\n",
    "\n",
    "    validity_id2label = {v: k for k, v in validity_label_mapping.items()}\n",
    "    novelty_id2label = {v: k for k, v in novelty_label_mapping.items()}\n",
    "\n",
    "    local_str_mapping = {\n",
    "        'novel': 1,\n",
    "        'not-novel': 0,\n",
    "        'valid': 1,\n",
    "        'not-valid': 0\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def val_nov_metric(is_validity: np.ndarray, should_validity: np.ndarray, is_novelty: np.ndarray,\n",
    "                       should_novelty: np.ndarray) -> Dict[str, float]:\n",
    "        ret = dict()\n",
    "\n",
    "        ret_base_help = {\n",
    "            \"true_positive_validity\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, should_validity >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_novelty\": np.sum(np.where(\n",
    "                np.all(np.stack([is_novelty >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5,\n",
    "                                 should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5,\n",
    "                                 should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5,\n",
    "                                 should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5,\n",
    "                                 should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_validity\": np.sum(np.where(is_validity >= .5, 1, 0)),\n",
    "            \"classified_positive_novelty\": np.sum(np.where(is_novelty >= .5, 1, 0)),\n",
    "            \"classified_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_validity\": np.sum(np.where(should_validity >= .5, 1, 0)),\n",
    "            \"indeed_positive_novelty\": np.sum(np.where(should_novelty >= .5, 1, 0)),\n",
    "            \"indeed_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "        }\n",
    "\n",
    "        ret_help = {\n",
    "            \"precision_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                                  max(1, ret_base_help[\"classified_positive_validity\"]),\n",
    "            \"precision_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                                 max(1, ret_base_help[\"classified_positive_novelty\"]),\n",
    "            \"recall_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                               max(1, ret_base_help[\"indeed_positive_validity\"]),\n",
    "            \"recall_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                              max(1, ret_base_help[\"indeed_positive_novelty\"]),\n",
    "            \"precision_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"classified_positive_valid_novel\"]),\n",
    "            \"precision_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_valid_nonnovel\"]),\n",
    "            \"precision_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_nonvalid_novel\"]),\n",
    "            \"precision_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                           max(1, ret_base_help[\"classified_positive_nonvalid_nonnovel\"]),\n",
    "            \"recall_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                  max(1, ret_base_help[\"indeed_positive_valid_novel\"]),\n",
    "            \"recall_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_valid_nonnovel\"]),\n",
    "            \"recall_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_nonvalid_novel\"]),\n",
    "            \"recall_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"indeed_positive_nonvalid_nonnovel\"])\n",
    "        }\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_validity\": 2 * ret_help[\"precision_validity\"] * ret_help[\"recall_validity\"] / max(1e-4, ret_help[\n",
    "                \"precision_validity\"] + ret_help[\"recall_validity\"]),\n",
    "            \"f1_novelty\": 2 * ret_help[\"precision_novelty\"] * ret_help[\"recall_novelty\"] / max(1e-4, ret_help[\n",
    "                \"precision_novelty\"] + ret_help[\"recall_novelty\"]),\n",
    "            \"f1_valid_novel\": 2 * ret_help[\"precision_valid_novel\"] * ret_help[\"recall_valid_novel\"] / max(1e-4,\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"precision_valid_novel\"] +\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"recall_valid_novel\"]),\n",
    "            \"f1_valid_nonnovel\": 2 * ret_help[\"precision_valid_nonnovel\"] * ret_help[\"recall_valid_nonnovel\"] / max(\n",
    "                1e-4, ret_help[\"precision_valid_nonnovel\"] + ret_help[\"recall_valid_nonnovel\"]),\n",
    "            \"f1_nonvalid_novel\": 2 * ret_help[\"precision_nonvalid_novel\"] * ret_help[\"recall_nonvalid_novel\"] / max(\n",
    "                1e-4, ret_help[\"precision_nonvalid_novel\"] + ret_help[\"recall_nonvalid_novel\"]),\n",
    "            \"f1_nonvalid_nonnovel\": 2 * ret_help[\"precision_nonvalid_nonnovel\"] * ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"] / max(1e-4, ret_help[\"precision_nonvalid_nonnovel\"] + ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"])\n",
    "        })\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_macro\": (ret[\"f1_valid_novel\"] + ret[\"f1_valid_nonnovel\"] + ret[\"f1_nonvalid_novel\"] + ret[\n",
    "                \"f1_nonvalid_nonnovel\"]) / 4\n",
    "        })\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(baseline_name: str, y_true: dict, y_pred: dict):\n",
    "    print(f\"==== {baseline_name} ====\")\n",
    "    print(\"Validity\")\n",
    "    results_validity = classification_report(\n",
    "        y_true['validity'],\n",
    "        y_pred['validity'],\n",
    "        target_names=['not-valid', 'valid'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_validity)\n",
    "\n",
    "    print(\"Novelty\")\n",
    "    results_novelty = classification_report(\n",
    "        y_true['novelty'],\n",
    "        y_pred['novelty'],\n",
    "        target_names=['not-novel', 'novel'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_novelty)\n",
    "\n",
    "    print(\"Combined (organization eval)\")\n",
    "    res = SharedTaskConstants.val_nov_metric(\n",
    "        np.array(y_pred['validity']),\n",
    "        np.array(y_true['validity']),\n",
    "        np.array(y_pred['novelty']),\n",
    "        np.array(y_true['novelty']),\n",
    "    )\n",
    "    print(res['f1_macro'].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(text):\n",
    "    '''\n",
    "    Return tensor string into list\n",
    "    '''\n",
    "    # clean string\n",
    "    clean_str = text.replace('tensor(', '').replace(')', '').strip()\n",
    "    # convert to list\n",
    "    tensor = eval(clean_str, {\"torch\": torch, \"__builtins__\": {}})\n",
    "    return tensor\n",
    "\n",
    "def process_covariate_data(df):\n",
    "    '''\n",
    "    Expanding all tensors in a single cell\n",
    "    Make confidence into ordinal variables\n",
    "    '''\n",
    "    # convert str to tensor (list)\n",
    "    SBERT_premise = df.SBERT_premise.apply(lambda x: str_to_list(x))\n",
    "    SBERT_conclusion = df.SBERT_conclusion.apply(lambda x: str_to_list(x))\n",
    "\n",
    "    # expand the list into individual entries\n",
    "    df_expand1 = SBERT_premise.apply(pd.Series)\n",
    "    df_expand2 = SBERT_conclusion.apply(pd.Series)\n",
    "\n",
    "    # assign a meaningful name\n",
    "    df_expand1.columns = ['pre_emb{}'.format(i+1) for i in range(df_expand1.shape[1])]\n",
    "    df_expand2.columns = ['con_emb{}'.format(i+1) for i in range(df_expand2.shape[1])]\n",
    "\n",
    "    # put everything together\n",
    "    df_final = pd.concat([df.drop(['SBERT_premise', \"SBERT_conclusion\"], axis=1), df_expand1, df_expand2], axis=1)\n",
    "    return df_final\n",
    "\n",
    "def preprocess_input(x, y):\n",
    "    '''\n",
    "    return DataLoader for later input into the model\n",
    "    '''\n",
    "    # pd.dataframe to array\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "    # transform y for nn model\n",
    "    # assume a value of 0 is not valid/novel\n",
    "    y[y == -1] = 0\n",
    "    # Transform the data\n",
    "    transformation_dict = {\n",
    "    (1, 1): [1, 0, 0, 0],\n",
    "    (1, 0): [0, 1, 0, 0],\n",
    "    (0, 1): [0, 0, 1, 0],\n",
    "    (0, 0): [0, 0, 0, 1],\n",
    "    }\n",
    "    \n",
    "    y = np.array([transformation_dict[tuple(row)] for row in y])\n",
    "    # array to tensor\n",
    "    x_torch = torch.tensor(x)\n",
    "    y_torch = torch.tensor(y)\n",
    "    data = TensorDataset(x_torch, y_torch)\n",
    "\n",
    "    batch_size = 10\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/TaskA_train_neural_kg.csv\", index_col=False)\n",
    "test = pd.read_pickle(\"../Data/TaskA_test_neural_kg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:,[\"SBERT_cosine_sim\",'Irrelevancy', 'AveDistance']]\n",
    "y_train = train.loc[:,[\"Validity\",'Novelty']]\n",
    "y_train = y_train.replace(-1, 0)\n",
    "\n",
    "X_test = test.loc[:,[\"SBERT_cosine_sim\",'Irrelevancy', 'AveDistance']]\n",
    "y_test = test.loc[:,[\"Validity\",'Novelty']]\n",
    "y_test = y_test.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN under Sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== MLP ====\n",
      "Validity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-valid       0.31      0.62      0.41       102\n",
      "       valid       0.88      0.66      0.75       418\n",
      "\n",
      "    accuracy                           0.65       520\n",
      "   macro avg       0.59      0.64      0.58       520\n",
      "weighted avg       0.76      0.65      0.68       520\n",
      "\n",
      "Novelty\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-novel       1.00      0.57      0.72       520\n",
      "       novel       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.57       520\n",
      "   macro avg       0.50      0.28      0.36       520\n",
      "weighted avg       1.00      0.57      0.72       520\n",
      "\n",
      "Combined (organization eval)\n",
      "0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_valid = MLPClassifier(solver='adam', hidden_layer_sizes=(2, 2), alpha=0.1, random_state=259, max_iter=1000)\n",
    "clf_valid.fit(X_train, y_train.Validity)\n",
    "\n",
    "clf_novel = MLPClassifier(solver='adam', hidden_layer_sizes=(2, 2), alpha=0.1, random_state=259, max_iter=1000)\n",
    "clf_novel.fit(X_train, y_train.Novelty)\n",
    "\n",
    "pred = {}\n",
    "pred[\"validity\"] = clf_valid.predict(np.array(X_test))\n",
    "pred[\"novelty\"] = clf_novel.predict(np.array(X_test))\n",
    "true_y = {\"validity\": list(y_test.Validity), \"novelty\": list(y_test.Novelty)}\n",
    "\n",
    "print_results(\"MLP\", pred, true_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROBERT Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train.topic + \" [SEP] \" + train.Premise + \" [SEP] \" + train.Conclusion\n",
    "test_input = test.topic + \" [SEP] \" + test.Premise + \" [SEP] \" + test.Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "def get_embeddings(model, text):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "    return sentence_embeddings.cpu().squeeze(0)\n",
    "\n",
    "# Get embedding\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"usc-isi/sbert-roberta-large-anli-mnli-snli\")\n",
    "# model = AutoModel.from_pretrained(\"usc-isi/sbert-roberta-large-anli-mnli-snli\")\n",
    "# print(\"Get SBERT embeddings for a datapoint\")\n",
    "# total_embedding = train_input.apply(lambda x: get_embeddings(model, x))\n",
    "# Embedding_Matrix = torch.stack(total_embedding.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding from the RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "encoded_X_train = tokenizer(train_input.tolist(), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "encoded_X_test = tokenizer(test_input.tolist(), padding=True, truncation=True, return_tensors='pt', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to tensors and create a DataLoader\n",
    "y_train_valid = torch.tensor([label for label in y_train.Validity])\n",
    "y_test_valid = torch.tensor([label for label in y_test.Validity])\n",
    "\n",
    "dataset_train = TensorDataset(encoded_X_train['input_ids'].squeeze(), encoded_X_train['attention_mask'].squeeze(), y_train_valid)\n",
    "trainloader = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=25)\n",
    "\n",
    "dataset_test = TensorDataset(encoded_X_test['input_ids'].squeeze(), encoded_X_test['attention_mask'].squeeze(), y_test_valid)\n",
    "testloader = DataLoader(dataset_test, sampler=RandomSampler(dataset_test), batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.8204604983329773\n",
      "epoch: 1, loss: 0.8237802982330322\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, att_mask, labels = batch\n",
    "        labels = labels.long().view(-1)\n",
    "        outputs = model(input_ids = inputs, attention_mask = att_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch: {epoch}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 128])\n",
      "torch.Size([25, 128])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    inputs, att_mask, labels = batch\n",
    "print(inputs.shape)\n",
    "print(att_mask.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# small sample prediction (all the same?)\n",
    "model.eval()\n",
    "input_ids = tokenizer([\"Hello, my dog is cute [ SEP ] dog is cute\", \"terrible day [ SEP ] day is bad\", \"Food is delicious [ SEP ] Food taste bad\"], padding=True, truncation=True, return_tensors='pt', max_length=512)[\"input_ids\"]  # Batch size 1\n",
    "mask = tokenizer([\"Hello, my dog is cute [ SEP ] dog is cute\", \"terrible day [ SEP ] day is bad\", \"Food is delicious [ SEP ] Food taste bad\"], padding=True, truncation=True, return_tensors='pt', max_length=512)[\"attention_mask\"]  # Batch size 1\n",
    "\n",
    "truelabels = torch.tensor([1,1,0]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids, mask)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n",
      "tensor([[0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087],\n",
      "        [0.2578, 0.0087]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        inputs, att_mask, labels = batch\n",
    "        outputs = model(input_ids = inputs, attention_mask = att_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        prob_valid = F.softmax(outputs.logits, dim=1)\n",
    "    print(prob_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
