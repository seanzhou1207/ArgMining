{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (F-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "class SharedTaskConstants:\n",
    "    \"\"\"\n",
    "    Use these constants to interface with the data, not with the id2label used\n",
    "    inside the Huggingface models!!\n",
    "    \"\"\"\n",
    "    targets = ['validity', 'novelty']\n",
    "    validity_label_mapping = {\n",
    "        -1: \"not-valid\",\n",
    "        0: \"not-valid\",  # can be excluded since test set does not contain these\n",
    "        1: \"valid\",\n",
    "    }\n",
    "\n",
    "    novelty_label_mapping = {\n",
    "        -1: \"not-novel\",\n",
    "        0: \"not-novel\",  # can be excluded since test set does not contain these\n",
    "        1: \"novel\",\n",
    "    }\n",
    "\n",
    "    validity_id2label = {v: k for k, v in validity_label_mapping.items()}\n",
    "    novelty_id2label = {v: k for k, v in novelty_label_mapping.items()}\n",
    "\n",
    "    local_str_mapping = {\n",
    "        'novel': 1,\n",
    "        'not-novel': 0,\n",
    "        'valid': 1,\n",
    "        'not-valid': 0\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def val_nov_metric(is_validity: np.ndarray, should_validity: np.ndarray, is_novelty: np.ndarray,\n",
    "                       should_novelty: np.ndarray) -> Dict[str, float]:\n",
    "        ret = dict()\n",
    "\n",
    "        ret_base_help = {\n",
    "            \"true_positive_validity\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, should_validity >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_novelty\": np.sum(np.where(\n",
    "                np.all(np.stack([is_novelty >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5,\n",
    "                                 should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5,\n",
    "                                 should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5,\n",
    "                                 should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5,\n",
    "                                 should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_validity\": np.sum(np.where(is_validity >= .5, 1, 0)),\n",
    "            \"classified_positive_novelty\": np.sum(np.where(is_novelty >= .5, 1, 0)),\n",
    "            \"classified_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_validity\": np.sum(np.where(should_validity >= .5, 1, 0)),\n",
    "            \"indeed_positive_novelty\": np.sum(np.where(should_novelty >= .5, 1, 0)),\n",
    "            \"indeed_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "        }\n",
    "\n",
    "        ret_help = {\n",
    "            \"precision_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                                  max(1, ret_base_help[\"classified_positive_validity\"]),\n",
    "            \"precision_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                                 max(1, ret_base_help[\"classified_positive_novelty\"]),\n",
    "            \"recall_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                               max(1, ret_base_help[\"indeed_positive_validity\"]),\n",
    "            \"recall_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                              max(1, ret_base_help[\"indeed_positive_novelty\"]),\n",
    "            \"precision_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"classified_positive_valid_novel\"]),\n",
    "            \"precision_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_valid_nonnovel\"]),\n",
    "            \"precision_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_nonvalid_novel\"]),\n",
    "            \"precision_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                           max(1, ret_base_help[\"classified_positive_nonvalid_nonnovel\"]),\n",
    "            \"recall_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                  max(1, ret_base_help[\"indeed_positive_valid_novel\"]),\n",
    "            \"recall_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_valid_nonnovel\"]),\n",
    "            \"recall_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_nonvalid_novel\"]),\n",
    "            \"recall_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"indeed_positive_nonvalid_nonnovel\"])\n",
    "        }\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_validity\": 2 * ret_help[\"precision_validity\"] * ret_help[\"recall_validity\"] / max(1e-4, ret_help[\n",
    "                \"precision_validity\"] + ret_help[\"recall_validity\"]),\n",
    "            \"f1_novelty\": 2 * ret_help[\"precision_novelty\"] * ret_help[\"recall_novelty\"] / max(1e-4, ret_help[\n",
    "                \"precision_novelty\"] + ret_help[\"recall_novelty\"]),\n",
    "            \"f1_valid_novel\": 2 * ret_help[\"precision_valid_novel\"] * ret_help[\"recall_valid_novel\"] / max(1e-4,\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"precision_valid_novel\"] +\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"recall_valid_novel\"]),\n",
    "            \"f1_valid_nonnovel\": 2 * ret_help[\"precision_valid_nonnovel\"] * ret_help[\"recall_valid_nonnovel\"] / max(\n",
    "                1e-4, ret_help[\"precision_valid_nonnovel\"] + ret_help[\"recall_valid_nonnovel\"]),\n",
    "            \"f1_nonvalid_novel\": 2 * ret_help[\"precision_nonvalid_novel\"] * ret_help[\"recall_nonvalid_novel\"] / max(\n",
    "                1e-4, ret_help[\"precision_nonvalid_novel\"] + ret_help[\"recall_nonvalid_novel\"]),\n",
    "            \"f1_nonvalid_nonnovel\": 2 * ret_help[\"precision_nonvalid_nonnovel\"] * ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"] / max(1e-4, ret_help[\"precision_nonvalid_nonnovel\"] + ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"])\n",
    "        })\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_macro\": (ret[\"f1_valid_novel\"] + ret[\"f1_valid_nonnovel\"] + ret[\"f1_nonvalid_novel\"] + ret[\n",
    "                \"f1_nonvalid_nonnovel\"]) / 4\n",
    "        })\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(baseline_name: str, y_true: dict, y_pred: dict):\n",
    "    print(f\"==== {baseline_name} ====\")\n",
    "    print(\"Validity\")\n",
    "    results_validity = classification_report(\n",
    "        y_true['validity'],\n",
    "        y_pred['validity'],\n",
    "        target_names=['not-valid', 'valid'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_validity)\n",
    "\n",
    "    print(\"Novelty\")\n",
    "    results_novelty = classification_report(\n",
    "        y_true['novelty'],\n",
    "        y_pred['novelty'],\n",
    "        target_names=['not-novel', 'novel'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_novelty)\n",
    "\n",
    "    print(\"Combined (organization eval)\")\n",
    "    res = SharedTaskConstants.val_nov_metric(\n",
    "        np.array(y_pred['validity']),\n",
    "        np.array(y_true['validity']),\n",
    "        np.array(y_pred['novelty']),\n",
    "        np.array(y_true['novelty']),\n",
    "    )\n",
    "    print(res['f1_macro'].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(text):\n",
    "    '''\n",
    "    Return tensor string into list\n",
    "    '''\n",
    "    # clean string\n",
    "    clean_str = text.replace('tensor(', '').replace(')', '').strip()\n",
    "    # convert to list\n",
    "    tensor = eval(clean_str, {\"torch\": torch, \"__builtins__\": {}})\n",
    "    return tensor\n",
    "\n",
    "def process_covariate_data(df):\n",
    "    '''\n",
    "    Expanding all tensors in a single cell\n",
    "    Make confidence into ordinal variables\n",
    "    '''\n",
    "    # convert str to tensor (list)\n",
    "    SBERT_premise = df.SBERT_premise.apply(lambda x: str_to_list(x))\n",
    "    SBERT_conclusion = df.SBERT_conclusion.apply(lambda x: str_to_list(x))\n",
    "\n",
    "    # expand the list into individual entries\n",
    "    df_expand1 = SBERT_premise.apply(pd.Series)\n",
    "    df_expand2 = SBERT_conclusion.apply(pd.Series)\n",
    "\n",
    "    # assign a meaningful name\n",
    "    df_expand1.columns = ['pre_emb{}'.format(i+1) for i in range(df_expand1.shape[1])]\n",
    "    df_expand2.columns = ['con_emb{}'.format(i+1) for i in range(df_expand2.shape[1])]\n",
    "\n",
    "    # put everything together\n",
    "    df_final = pd.concat([df.drop(['SBERT_premise', \"SBERT_conclusion\"], axis=1), df_expand1, df_expand2], axis=1)\n",
    "    return df_final\n",
    "\n",
    "def preprocess_input(x, y):\n",
    "    '''\n",
    "    return DataLoader for later input into the model\n",
    "    '''\n",
    "    # pd.dataframe to array\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "    # transform y for nn model\n",
    "    # assume a value of 0 is not valid/novel\n",
    "    y[y == -1] = 0\n",
    "    # Transform the data\n",
    "    transformation_dict = {\n",
    "    (1, 1): [1, 0, 0, 0],\n",
    "    (1, 0): [0, 1, 0, 0],\n",
    "    (0, 1): [0, 0, 1, 0],\n",
    "    (0, 0): [0, 0, 0, 1],\n",
    "    }\n",
    "    \n",
    "    y = np.array([transformation_dict[tuple(row)] for row in y])\n",
    "    # array to tensor\n",
    "    x_torch = torch.tensor(x)\n",
    "    y_torch = torch.tensor(y)\n",
    "    data = TensorDataset(x_torch, y_torch)\n",
    "\n",
    "    batch_size = 10\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/TaskA_train_neural_kg.csv\", index_col=False)\n",
    "test = pd.read_pickle(\"../Data/TaskA_test_neural_kg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:,[\"SBERT_cosine_sim\",'Irrelevancy', 'AveDistance']]\n",
    "y_train = train.loc[:,[\"Validity\",'Novelty']]\n",
    "y_train = y_train.replace(-1, 0)\n",
    "\n",
    "X_test = test.loc[:,[\"SBERT_cosine_sim\",'Irrelevancy', 'AveDistance']]\n",
    "y_test = test.loc[:,[\"Validity\",'Novelty']]\n",
    "y_test = y_test.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN under Sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== MLP ====\n",
      "Validity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-valid       0.31      0.62      0.41       102\n",
      "       valid       0.88      0.66      0.75       418\n",
      "\n",
      "    accuracy                           0.65       520\n",
      "   macro avg       0.59      0.64      0.58       520\n",
      "weighted avg       0.76      0.65      0.68       520\n",
      "\n",
      "Novelty\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-novel       1.00      0.57      0.72       520\n",
      "       novel       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.57       520\n",
      "   macro avg       0.50      0.28      0.36       520\n",
      "weighted avg       1.00      0.57      0.72       520\n",
      "\n",
      "Combined (organization eval)\n",
      "0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_valid = MLPClassifier(solver='adam', hidden_layer_sizes=(2, 2), alpha=0.1, random_state=259, max_iter=1000)\n",
    "clf_valid.fit(X_train, y_train.Validity)\n",
    "\n",
    "clf_novel = MLPClassifier(solver='adam', hidden_layer_sizes=(2, 2), alpha=0.1, random_state=259, max_iter=1000)\n",
    "clf_novel.fit(X_train, y_train.Novelty)\n",
    "\n",
    "pred = {}\n",
    "pred[\"validity\"] = clf_valid.predict(np.array(X_test))\n",
    "pred[\"novelty\"] = clf_novel.predict(np.array(X_test))\n",
    "true_y = {\"validity\": list(y_test.Validity), \"novelty\": list(y_test.Novelty)}\n",
    "\n",
    "print_results(\"MLP\", pred, true_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
