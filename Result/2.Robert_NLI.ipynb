{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/TaskA_train_neural.csv\", index_col=False)\n",
    "test = pd.read_csv(\"../Data/TaskA_test_neural.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:, [\"topic\", \"Premise\", \"Conclusion\", \"SBERT_premise\", \"SBERT_conclusion\", \"SBERT_cosine_sim\"]]\n",
    "y_train = train.loc[:, [\"Validity\", \"Novelty\"]]\n",
    "X_test = test.loc[:, [\"topic\", \"Premise\", \"Conclusion\", \"SBERT_premise\", \"SBERT_conclusion\", \"SBERT_cosine_sim\"]]\n",
    "y_test = test.loc[:, [\"Validity\", \"Novelty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(text):\n",
    "    '''\n",
    "    Return tensor string into list\n",
    "    '''\n",
    "    # clean string\n",
    "    clean_str = text.replace('tensor(', '').replace(')', '').strip()\n",
    "    # convert to list\n",
    "    tensor = eval(clean_str, {\"torch\": torch, \"__builtins__\": {}})\n",
    "    return tensor\n",
    "\n",
    "def process_covariate_data(df):\n",
    "    '''\n",
    "    Expanding all tensors in a single cell\n",
    "    '''\n",
    "    # convert str to tensor (list)\n",
    "    SBERT_premise = df.SBERT_premise.apply(lambda x: str_to_list(x))\n",
    "    SBERT_conclusion = df.SBERT_conclusion.apply(lambda x: str_to_list(x))\n",
    "\n",
    "    # expand the list into individual entries\n",
    "    df_expand1 = SBERT_premise.apply(pd.Series)\n",
    "    df_expand2 = SBERT_conclusion.apply(pd.Series)\n",
    "\n",
    "    # assign a meaningful name\n",
    "    df_expand1.columns = ['pre_emb{}'.format(i+1) for i in range(df_expand1.shape[1])]\n",
    "    df_expand2.columns = ['con_emb{}'.format(i+1) for i in range(df_expand2.shape[1])]\n",
    "\n",
    "    # put everything together\n",
    "    df_final = pd.concat([df.drop(['SBERT_premise', \"SBERT_conclusion\"], axis=1), df_expand1, df_expand2], axis=1)\n",
    "    return df_final\n",
    "\n",
    "def preprocess_input(x, y):\n",
    "    '''\n",
    "    return DataLoader for later input into the model\n",
    "    '''\n",
    "    # pd.dataframe to array\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "    # transform y for nn model\n",
    "    # assume a value of 0 is not valid/novel\n",
    "    y[y == -1] = 0\n",
    "    # Transform the data\n",
    "    transformation_dict = {\n",
    "    (1, 1): [1, 0, 0, 0],\n",
    "    (1, 0): [0, 1, 0, 0],\n",
    "    (0, 1): [0, 0, 1, 0],\n",
    "    (0, 0): [0, 0, 0, 1],\n",
    "    }\n",
    "    \n",
    "    y = np.array([transformation_dict[tuple(row)] for row in y])\n",
    "    # array to tensor\n",
    "    x_torch = torch.tensor(x)\n",
    "    y_torch = torch.tensor(y)\n",
    "    data = TensorDataset(x_torch, y_torch)\n",
    "\n",
    "    batch_size = 10\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.input_dim = 1537\n",
    "        self.hidden_dim = 200\n",
    "        self.output_dim = 4\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)  # Assuming n input features\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)  # Batch normalization layer\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)  # Outputs 4 values, one for each output column\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = process_covariate_data(X_train)\n",
    "X_test1 = process_covariate_data(X_test)\n",
    "\n",
    "train_loader = preprocess_input(X_train1.iloc[:, 3:], y_train)\n",
    "test_loader = preprocess_input(X_test1.iloc[:, 3:], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15527\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1268659830093384\n",
      "Epoch 11, Loss: 1.2287373542785645\n",
      "Epoch 21, Loss: 1.0439461469650269\n",
      "Epoch 31, Loss: 1.1889241933822632\n",
      "Epoch 41, Loss: 1.0718261003494263\n",
      "Epoch 51, Loss: 0.895089328289032\n"
     ]
    }
   ],
   "source": [
    "epochs = 51\n",
    "for epoch in range(epochs):\n",
    "    # for each batch\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Optionally print the loss every few epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.loc[y_test.Validity == -1, \"Validity\"] = 0\n",
    "y_test.loc[y_test.Novelty == -1, \"Novelty\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_label = {\"validity\":list(y_test.Validity), \"novelty\":list(y_test.Novelty)}\n",
    "test_preds = {\"validity\":[], \"novelty\": []}\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = F.one_hot(predicted, num_classes=4)\n",
    "        # Transform the data\n",
    "        transformation_dict = {\n",
    "        (1, 0, 0, 0): (1, 1), \n",
    "        (0, 1, 0, 0): (1, 0),\n",
    "        (0, 0, 1, 0): (0, 1),\n",
    "        (0, 0, 0, 1): (0, 1)\n",
    "        }\n",
    "        predicted = [transformation_dict[tuple(row.tolist())] for row in predicted]\n",
    "        # Obtain the prediction\n",
    "        for pred in predicted:\n",
    "            test_preds[\"validity\"].append(pred[0])\n",
    "            test_preds[\"novelty\"].append(pred[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "class SharedTaskConstants:\n",
    "    \"\"\"\n",
    "    Use these constants to interface with the data, not with the id2label used\n",
    "    inside the Huggingface models!!\n",
    "    \"\"\"\n",
    "    targets = ['validity', 'novelty']\n",
    "    validity_label_mapping = {\n",
    "        -1: \"not-valid\",\n",
    "        0: \"not-valid\",  # can be excluded since test set does not contain these\n",
    "        1: \"valid\",\n",
    "    }\n",
    "\n",
    "    novelty_label_mapping = {\n",
    "        -1: \"not-novel\",\n",
    "        0: \"not-novel\",  # can be excluded since test set does not contain these\n",
    "        1: \"novel\",\n",
    "    }\n",
    "\n",
    "    validity_id2label = {v: k for k, v in validity_label_mapping.items()}\n",
    "    novelty_id2label = {v: k for k, v in novelty_label_mapping.items()}\n",
    "\n",
    "    local_str_mapping = {\n",
    "        'novel': 1,\n",
    "        'not-novel': 0,\n",
    "        'valid': 1,\n",
    "        'not-valid': 0\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def val_nov_metric(is_validity: np.ndarray, should_validity: np.ndarray, is_novelty: np.ndarray,\n",
    "                       should_novelty: np.ndarray) -> Dict[str, float]:\n",
    "        ret = dict()\n",
    "\n",
    "        ret_base_help = {\n",
    "            \"true_positive_validity\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, should_validity >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_novelty\": np.sum(np.where(\n",
    "                np.all(np.stack([is_novelty >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5,\n",
    "                                 should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5,\n",
    "                                 should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5,\n",
    "                                 should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"true_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5,\n",
    "                                 should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_validity\": np.sum(np.where(is_validity >= .5, 1, 0)),\n",
    "            \"classified_positive_novelty\": np.sum(np.where(is_novelty >= .5, 1, 0)),\n",
    "            \"classified_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity >= .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"classified_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([is_validity < .5, is_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_validity\": np.sum(np.where(should_validity >= .5, 1, 0)),\n",
    "            \"indeed_positive_novelty\": np.sum(np.where(should_novelty >= .5, 1, 0)),\n",
    "            \"indeed_positive_valid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_novel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_valid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "            \"indeed_positive_nonvalid_nonnovel\": np.sum(np.where(\n",
    "                np.all(np.stack([should_validity < .5, should_novelty < .5]), axis=0),\n",
    "                1, 0)),\n",
    "        }\n",
    "\n",
    "        ret_help = {\n",
    "            \"precision_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                                  max(1, ret_base_help[\"classified_positive_validity\"]),\n",
    "            \"precision_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                                 max(1, ret_base_help[\"classified_positive_novelty\"]),\n",
    "            \"recall_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                               max(1, ret_base_help[\"indeed_positive_validity\"]),\n",
    "            \"recall_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                              max(1, ret_base_help[\"indeed_positive_novelty\"]),\n",
    "            \"precision_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"classified_positive_valid_novel\"]),\n",
    "            \"precision_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_valid_nonnovel\"]),\n",
    "            \"precision_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                        max(1, ret_base_help[\"classified_positive_nonvalid_novel\"]),\n",
    "            \"precision_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                           max(1, ret_base_help[\"classified_positive_nonvalid_nonnovel\"]),\n",
    "            \"recall_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                  max(1, ret_base_help[\"indeed_positive_valid_novel\"]),\n",
    "            \"recall_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_valid_nonnovel\"]),\n",
    "            \"recall_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                     max(1, ret_base_help[\"indeed_positive_nonvalid_novel\"]),\n",
    "            \"recall_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                        max(1, ret_base_help[\"indeed_positive_nonvalid_nonnovel\"])\n",
    "        }\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_validity\": 2 * ret_help[\"precision_validity\"] * ret_help[\"recall_validity\"] / max(1e-4, ret_help[\n",
    "                \"precision_validity\"] + ret_help[\"recall_validity\"]),\n",
    "            \"f1_novelty\": 2 * ret_help[\"precision_novelty\"] * ret_help[\"recall_novelty\"] / max(1e-4, ret_help[\n",
    "                \"precision_novelty\"] + ret_help[\"recall_novelty\"]),\n",
    "            \"f1_valid_novel\": 2 * ret_help[\"precision_valid_novel\"] * ret_help[\"recall_valid_novel\"] / max(1e-4,\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"precision_valid_novel\"] +\n",
    "                                                                                                           ret_help[\n",
    "                                                                                                               \"recall_valid_novel\"]),\n",
    "            \"f1_valid_nonnovel\": 2 * ret_help[\"precision_valid_nonnovel\"] * ret_help[\"recall_valid_nonnovel\"] / max(\n",
    "                1e-4, ret_help[\"precision_valid_nonnovel\"] + ret_help[\"recall_valid_nonnovel\"]),\n",
    "            \"f1_nonvalid_novel\": 2 * ret_help[\"precision_nonvalid_novel\"] * ret_help[\"recall_nonvalid_novel\"] / max(\n",
    "                1e-4, ret_help[\"precision_nonvalid_novel\"] + ret_help[\"recall_nonvalid_novel\"]),\n",
    "            \"f1_nonvalid_nonnovel\": 2 * ret_help[\"precision_nonvalid_nonnovel\"] * ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"] / max(1e-4, ret_help[\"precision_nonvalid_nonnovel\"] + ret_help[\n",
    "                \"recall_nonvalid_nonnovel\"])\n",
    "        })\n",
    "\n",
    "        ret.update({\n",
    "            \"f1_macro\": (ret[\"f1_valid_novel\"] + ret[\"f1_valid_nonnovel\"] + ret[\"f1_nonvalid_novel\"] + ret[\n",
    "                \"f1_nonvalid_nonnovel\"]) / 4\n",
    "        })\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(baseline_name: str, y_true: dict, y_pred: dict):\n",
    "    print(f\"==== {baseline_name} ====\")\n",
    "    print(\"Validity\")\n",
    "    results_validity = classification_report(\n",
    "        y_true['validity'],\n",
    "        y_pred['validity'],\n",
    "        target_names=['not-valid', 'valid'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_validity)\n",
    "\n",
    "    print(\"Novelty\")\n",
    "    results_novelty = classification_report(\n",
    "        y_true['novelty'],\n",
    "        y_pred['novelty'],\n",
    "        target_names=['not-novel', 'novel'],\n",
    "        labels=[0, 1],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(results_novelty)\n",
    "\n",
    "    print(\"Combined (organization eval)\")\n",
    "    res = SharedTaskConstants.val_nov_metric(\n",
    "        np.array(y_pred['validity']),\n",
    "        np.array(y_true['validity']),\n",
    "        np.array(y_pred['novelty']),\n",
    "        np.array(y_true['novelty']),\n",
    "    )\n",
    "    print(res['f1_macro'].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Roberta_NLI ====\n",
      "Validity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-valid       0.40      0.70      0.51       206\n",
      "       valid       0.61      0.30      0.40       314\n",
      "\n",
      "    accuracy                           0.46       520\n",
      "   macro avg       0.50      0.50      0.45       520\n",
      "weighted avg       0.52      0.46      0.44       520\n",
      "\n",
      "Novelty\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not-novel       0.52      0.24      0.33       294\n",
      "       novel       0.42      0.71      0.53       226\n",
      "\n",
      "    accuracy                           0.45       520\n",
      "   macro avg       0.47      0.48      0.43       520\n",
      "weighted avg       0.48      0.45      0.42       520\n",
      "\n",
      "Combined (organization eval)\n",
      "0.1638\n"
     ]
    }
   ],
   "source": [
    "print_results(\"Roberta_NLI\", test_label, test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
